# -*- coding: utf-8 -*-
"""Previsão de Investimentos Bancários com Machine Learning usando Decison Tree

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1kkMh4hhhic885UMXaovONeL4QyPQl1v6

# Previsão de Investimentos Bancários com Machine Learning usando Decison Tree:

Neste projeto, iremos analisar dados de uma campanha de marketing para aderência de investimentos. O intuito é usar as informações dos dados para prever se os clientes de um banco vão aplicar o dinheiro em um investimento ou não.

Essa previsão será feita utilizando machine learning e este notebook vai conter os passos para se obter um modelo capaz de realizar as previsões, desde a leitura, análise exploratória, separação e transformação dos dados, até o ajuste, avaliação e comparação de modelos de classificação
"""

# Problema de negócio: Quais clientes vão aderir ao investimento do banco?

# Dados anteriores com características dos clientes e resultados para classificar eles em aderindo ou não.

"""# 1. Análise exploratória dos dados:"""

# Importando as bibliotcas que iremos usar:

import pandas as pd
import plotly.express as px

# 1. Upload do arquivo e leitura dos dados:

# Link: https://cdn3.gnarususercontent.com.br/3067-classificacao/Projeto/dados/marketing_investimento.csv

dados = pd.read_csv('/content/marketing_investimento.csv')
dados.head()

# 2. Conhecendo um pouco nossos dados

dados.describe()

# Verrificar se tem alguma informação faltante:

dados.info()

# Em todas as colunas tem 1268 dados não nunos, ou seja, não precisaremos fazer tratamentos para tratar essa base.

dados.columns

"""Idade, saldo, tempo_ult_contato e numero_contatos são variáveis numéricas.

Estado_civil, escolaridade, inadimplencia, fez_emprestimo e aderencia_investimento são categóricas.

A coluna aderencia_investimento é a mais importante para que possamos obter as nossas respostas, mas ela não pode ser categórica, pois o algoritmo não entende.

## O fato da coluna alvo ser categória nos indica que o modelo de classificação é uma boa abordagem para solucionarmos esse problema!

## Variáveis categóricas:
"""

# Verrificando se tem alguma inconsistência nos nossos dados de forma mais vizual:

px.histogram(dados, x ='aderencia_investimento', text_auto = True, color = 'aderencia_investimento')

px.histogram(dados, x ='estado_civil', text_auto = True, color = 'aderencia_investimento', barmode = 'group')

px.histogram(dados, x ='escolaridade', text_auto = True, color = 'aderencia_investimento', barmode = 'group')

px.histogram(dados, x ='fez_emprestimo', text_auto = True, color = 'aderencia_investimento', barmode = 'group')

px.histogram(dados, x ='inadimplencia', text_auto = True, color = 'aderencia_investimento', barmode = 'group')

"""Verrificamos que todas as colunas estão dentro do esparado, sem categorias erradas.

## Variáveis numéricas:
"""

px.box(dados, x = 'idade', color = 'aderencia_investimento')

"""Note que não existe uma diferença muito grande entre a distribuição dessa variável para cada uma dessas categorias. O boxplot dos dois é bem parecido. O valor mínimo na base de dados de idade é 19 e o valor máximo é 87. Portanto, parece que não existem inconsistências com essa coluna. Não encontramos valores menores do que 18 anos, que indicariam pessoas que não poderiam fazer investimento, nem valores negativos, que seriam impossíveis.

"""

px.box(dados, x = 'saldo', color = 'aderencia_investimento')

"""Note que temos muito mais pontos considerados outliers (ponto fora da curva). São valores discrepantes do padrão geral daqueles dados. Mas, se observarmos, as duas caixas não tem uma diferença tão grande na distribuição da aderência de investimento. Mesmo assim, é possível perceber que o valor do saldo está mais concentrado para valores menores de saldo.

O valor mínimo do saldo é, no boxplot azul, é menos 1.206. Neste caso, faz sentido ter um valor negativo, pois estamos trabalhando com saldo bancário. Já o valor máximo é de 27.069.


"""

px.box(dados, x = 'tempo_ult_contato', color = 'aderencia_investimento')

"""Para esta coluna, conseguimos ver que tem uma diferença na distribuição dos valores. No boxplot vermelho, que se refere a não aderência do investimento, temos valores mais concentrados em tempos menores. Já no azul, de aderência ao investimento, há valores maiores do tempo.

O tempo de último contato é no mínimo 5. Como estamos lidando com tempo, não é possível haver valores negativos, o que realmente não encontramos, então não há inconsistência. Ainda assim, há uma diferença na distribuição que pode ser importante para definirmos se a pessoa vai aderir ao investimento ou não.
"""

px.box(dados, x = 'numero_contatos', color = 'aderencia_investimento')

"""Desta vez, conseguimos visualizar dois boxplots semelhantes, onde em ambos o valor mínimo é 1. Novamente não parece ter nenhuma inconsistência com esses dados, portanto, não precisaremos fazer nenhum tratamento em relação a elas.

# 2. Separando as variáveis

Precisamos deixar claro quem é a variável alvo (y) e as variáveis explicativas (x)! A alvo é o que queremos prever: aderencia_investimento, e as que vamos usar como input (explicativas) são todas as outras, vamos separar todas elas:
"""

# Criando essas variáveis:

x = dados.drop('aderencia_investimento', axis = 1)
y = dados['aderencia_investimento']

x # Todas as explicativas

y # Variável alvo

"""Novamente as informações estão em forma escrita, isso o algoritmo não entende, então temos que passar elas para forma numérica. Para isso vamos usar o One Hot Encoding."""

# Importando as bibliotecas:

from sklearn.compose import make_column_transformer
from sklearn.preprocessing import OneHotEncoder

"""## Transformando as variáveis explicativas"""

# Transformando variáveis explicativas:

colunas = x.columns

one_hot = make_column_transformer((
    OneHotEncoder(drop = 'if_binary'),
    ['estado_civil', 'escolaridade', 'inadimplencia', 'fez_emprestimo']
),
    remainder = 'passthrough',
    sparse_threshold = 0
    )

x = one_hot.fit_transform(x)

# Vendo as colunas que ele gerou:

one_hot.get_feature_names_out(colunas)

# Convetendo em data frame:

pd.DataFrame(x, columns = one_hot.get_feature_names_out(colunas))

"""## Transformando a variável alvo:

"""

# Vamos fazer isso usando o Label Enconder para variáveis alvo!

from sklearn.preprocessing import LabelEncoder

label_encoder = LabelEncoder()

y = label_encoder.fit_transform(y)

y

"""Agora nosso modelo esta pronto para inputar esses dados e aprender com eles.

# 3. Dividindo treino e teste:
"""

from sklearn.model_selection import train_test_split

# Dividindo tanto x quanto y em treino e teste:

x_treino, x_teste, y_treino, y_teste = train_test_split(x, y, stratify = y, random_state = 5)

"""#4. Criação de modelos:

### Dummy Classifier
"""

# Agora que ja separamos a base em treino-teste, vamos iniciar a modelagem com o modelo mais simples (dummy classifier)

# Esse algoritmo é importante ppara termos um modelo de comparação contra outros:

from sklearn.dummy import DummyClassifier

dummy = DummyClassifier()
dummy.fit(x_treino, y_treino)

dummy.score(x_teste, y_teste)

"""Em 60% das vezes o modelo fez a classificação de forma correta, então nosso modelo a seguir terá que ser melhor do que esse, tendo em vista que esse é o mais básico!

### Decision Tree:
"""

from sklearn.tree import DecisionTreeClassifier

arvore = DecisionTreeClassifier(random_state = 5)
arvore.fit(x_treino, y_treino)

# Fazendo previsão com o modelo:

arvore.predict(x_teste)

# Quais foram as escolhas para essa tomada de decisões:

from sklearn.tree import plot_tree
import matplotlib.pyplot as plt

# Mudando o nome das colunas para ficar mais fácil de entender:

nome_colunas = ['casado (a)',
                'divorciado (a)',
                'solteiro (a)',
                'fundamental',
                'medio',
                'superior',
                'inadimplencia',
                'fez_emprestimo',
                'idade',
                'saldo',
                'tempo_ult_contato',
                'numero_contatos']

plt.figure(figsize = (15,8))
plot_tree(arvore, filled = True, class_names = ['não', 'sim'], fontsize = 1, feature_names = nome_colunas);

"""Não da para ver muito bem, mas podemos chegar a conclusão de que foram tomadas muitas decisões, isso pode indicar que ele está decorrando os padrões."""

# Qual o score dessa previsão:

arvore.score(x_teste, y_teste)

# Como ele está no treino?
arvore.score(x_treino, y_treino)

"""Ou seja, o que estamos vendo é que ele decorrou o padrão dos dados de treino, e não aprendeu, por isso a quantidade grande de divisões. Para que isso ocorra temos que colocar como parametro a profundidade máxima da árvore (max_depth).

O resultado é de 66% de acerto, é melhor que o Dummy, mas ainda podemos melhorar e muito!
"""

arvore = DecisionTreeClassifier(random_state = 5, max_depth = 3)
arvore.fit(x_treino, y_treino)

# Qual o score dessa previsão:

arvore.score(x_treino, y_treino)

"""Vemos que agora o score para treino já esta mais adequado, chegando aos 76%."""

# E em teste:

arvore.score(x_teste, y_teste)

"""Tivemos um resultado de 71%, quando fazemos essa "poda", ou seja, ele está entendendo melhor esse padrão de comportamento dos dados. Vamos ver agora as decisões tomadas:"""

plt.figure(figsize = (15,8))
plot_tree(arvore, filled = True, class_names = ['não', 'sim'], fontsize = 7, feature_names = nome_colunas);

""" ### KNN (K-nearest neighbors)

 Para podermos usar o KNN, tendo em vista que ele lida com distâncias, primeiro teremos que normalizar esses dados. Nomalizar os dados é um processo que coloca eles todos na mesma escala, para que os pesos não influenciem. Por exemplo, se estivesses comparando idade x saldo, a idade é muito menor, então acabaria tendo menos peso, e não queremos que tenha esse viés na previsão. Aqui vamos usar a Min_max_scale para isso.
"""

from sklearn.preprocessing import MinMaxScaler

normalizado = MinMaxScaler()

x_treino_normalizado = normalizado.fit_transform(x_treino)

# Vendo o resultado em data frame:

pd.DataFrame(x_treino_normalizado)

"""Repare que agora todos eles estão na mesma escala, e isso permite com que possamos usar o KNN."""

from sklearn.neighbors import KNeighborsClassifier

knn = KNeighborsClassifier() # Padrão de K igual a 3

knn.fit(x_treino_normalizado, y_treino) # Importante passar os dados normalizados

x_teste_normalizado = normalizado.transform(x_teste)

knn.score(x_teste_normalizado, y_teste)

"""Resultado de 68%, que é melhor que o Dummy mas perde para o Decision Tree.

### Comparativo KNN x Dummy x Decison Tree:
"""

print(f'Acurácia Dummy: {dummy.score(x_teste, y_teste)}')
print(f'Acurácia Árvore: {arvore.score(x_teste, y_teste)}')
print(f'Acurácia KNN: {knn.score(x_teste_normalizado, y_teste)}')

"""## 5. Melhorando nosso melhor modelo - Decision Three:

5.1 Ajustando os hiperparâmetros:
"""

# Qual é o nosso melhor valor de max_depth?

from sklearn.model_selection import GridSearchCV
from sklearn.tree import DecisionTreeClassifier
from sklearn.metrics import accuracy_score

# Definir a faixa de valores de max_depth a ser testada
param_grid = {
    'max_depth': range(1, 20)
}

# Criar o modelo de árvore de decisão
arvore = DecisionTreeClassifier(random_state=5)

# Configurar o GridSearchCV
grid_search = GridSearchCV(arvore, param_grid, cv=5, scoring='accuracy')

# Ajustar o modelo aos dados de treino
grid_search.fit(x_treino, y_treino)

# Exibir o melhor parâmetro de max_depth
print(f"Melhor valor de max_depth: {grid_search.best_params_['max_depth']}")
print(f"Melhor acurácia na validação cruzada: {grid_search.best_score_}")

# Treinar o modelo com o melhor max_depth encontrado
arvore_melhorado = grid_search.best_estimator_
arvore_melhorado.fit(x_treino, y_treino)

# Avaliar no conjunto de teste
y_pred = arvore_melhorado.predict(x_teste)
print(f"Acurácia no conjunto de teste: {accuracy_score(y_teste, y_pred)}")

"""De fato, o melhor valor é o de 3, que já estamos usando!"""

# Ajustando o min_sample_split

from sklearn.model_selection import GridSearchCV
from sklearn.tree import DecisionTreeClassifier
from sklearn.metrics import accuracy_score

# Definir a faixa de valores de min_samples_split a ser testada
param_grid = {
    'min_samples_split': range(2, 21)
}

# Criar o modelo de árvore de decisão com max_depth fixo em 3
arvore = DecisionTreeClassifier(random_state=5, max_depth=3)

# Configurar o GridSearchCV para testar diferentes valores de min_samples_split
grid_search = GridSearchCV(arvore, param_grid, cv=5, scoring='accuracy')

# Ajustar o modelo aos dados de treino
grid_search.fit(x_treino, y_treino)

# Exibir o melhor parâmetro de min_samples_split
print(f"Melhor valor de min_samples_split: {grid_search.best_params_['min_samples_split']}")
print(f"Melhor acurácia na validação cruzada: {grid_search.best_score_}")

# Treinar o modelo com o melhor min_samples_split encontrado
arvore_melhorado = grid_search.best_estimator_
arvore_melhorado.fit(x_treino, y_treino)

# Avaliar no conjunto de teste
y_pred = arvore_melhorado.predict(x_teste)
print(f"Acurácia no conjunto de teste: {accuracy_score(y_teste, y_pred)}")

"""Com isso chegamos ao valor de min_samples_split de 2! Poderiamos ajustar outros, mas por hora esses são o bastante!"""

from sklearn.model_selection import cross_val_score
from sklearn.tree import DecisionTreeClassifier
import numpy as np

# Executar a validação cruzada (5-Fold)
scores = cross_val_score(arvore, x_treino, y_treino, cv=5, scoring='accuracy')

# Mostrar os resultados
print(f"Acurácias obtidas em cada fold: {scores}")
print(f"Média das acurácias: {np.mean(scores)}")
print(f"Desvio padrão das acurácias: {np.std(scores)}")

"""Com isso temos uma média de 74% de acerto no conjunto de testes!

## 6. Exportando o melhor modelo - Decision Three:

Precisamos exportar o modelo para usar fora de ambientes que não sejam o google collab!
"""

# Para isso vamos usar o picke!
import pickle

# Vamos armazenar também o One Hot enconder:

with open('modelo_onehorenc.pkl', 'wb')  as arquivo: #wb = write bytes
  pickle.dump(one_hot, arquivo)

"""Agora podemos usar esse modelo em quaisquer outros ambientes:"""

# Vamos armazenar a árvore:

with open('modelo_arvore.pkl', 'wb')  as arquivo: #wb = write bytes
  pickle.dump(arvore, arquivo)

"""Os dados no nosso modelo usado tem o one_hot_enconder, mas no sistema do banco estão em formato convencional, então temos que fazer essa transformação primeiro. Isso vale para novos dados, por exemplo:"""

dados

# Colocando um novo dado para testar:
novo_dado = {
    'idade': [45],
    'estado_civil':['solteiro (a)'],
    'escolaridade':['superior'],
    'inadimplencia': ['nao'],
    'saldo': [23040],
    'fez_emprestimo': ['nao'],
    'tempo_ult_contato': [800],
    'numero_contatos': [4]
}

# vamos simular a leitura do arquivo no sistema do banco:

modelo_one_hot = pd.read_pickle('/content/modelo_onehorenc.pkl')
modelo_arvore = pd.read_pickle('/content/modelo_arvore.pkl')

# Convetendo novo dado em data frame:

novo_dado = pd.DataFrame(novo_dado)

# Transformação do novo dado:

novo_dado = modelo_one_hot.transform(novo_dado)

modelo_arvore.predict(novo_dado)